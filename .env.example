# =============================================================================
# Databricks Configuration
# =============================================================================

# METHOD 1: Using Databricks CLI (RECOMMENDED)
# Configure using: databricks configure --profile DEFAULT
# The CLI will store credentials securely in ~/.databrickscfg
DATABRICKS_PROFILE=DEFAULT

# METHOD 2: Using Environment Variables (Alternative)
# Uncomment these if not using CLI profiles
# DATABRICKS_HOST=dbc-xxxxx-xxxx.cloud.databricks.com
# DATABRICKS_TOKEN=dapiXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# MLflow Configuration
MLFLOW_TRACKING_URI=databricks

# =============================================================================
# Spark Connect Configuration (for local development)
# =============================================================================

# Spark Connect is ENABLED BY DEFAULT for local development
# This allows you to run code locally while executing on remote Databricks cluster
# Set to "false" to use local Spark instead
# USE_SPARK_CONNECT=false

# Databricks Cluster ID for Spark Connect (REQUIRED)
# Find this in your Databricks workspace:
#   1. Go to Compute > Select your cluster
#   2. Copy the Cluster ID from the URL or Configuration tab
#   Example: 1234-567890-abcdefgh
SPARK_CONNECT_CLUSTER_ID=1234-567890-abcdefgh

# Optional: Custom Spark application name
SPARK_APP_NAME=entity-matching-pipeline

# =============================================================================
# Model Paths
# =============================================================================

# Path to trained Ditto model (optional)
DITTO_MODEL_PATH=models/ditto_entity_matcher
